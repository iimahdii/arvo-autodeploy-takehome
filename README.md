# ğŸš€ AutoDeploy - Automated Cloud Deployment System

**Intelligent, natural language-driven application deployment to AWS, GCP, and Azure**

AutoDeploy is a sophisticated backend system that automates the entire process of deploying applications to cloud infrastructure. Simply provide a natural language description and a GitHub repository, and AutoDeploy handles everything from code analysis to infrastructure provisioning to application deployment.

## ğŸ“‹ Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Usage Examples](#usage-examples)
- [How It Works](#how-it-works)
- [Technical Deep Dive](#technical-deep-dive)
- [Configuration](#configuration)
- [Supported Technologies](#supported-technologies)
- [Cost Estimation](#cost-estimation)
- [Troubleshooting](#troubleshooting)
- [Development](#development)

## âœ¨ Features

### Core Capabilities

- **ğŸ§  Intelligent Repository Analysis**: Automatically detects application type, framework, dependencies, and deployment requirements
- **ğŸ’¬ Natural Language Processing**: Multi-tier NLP with Vertex AI (Gemini), OpenAI, Anthropic, and rule-based fallback
- **ğŸ—ï¸ Smart Infrastructure Decisions**: Determines optimal deployment strategy (VM, containers, Kubernetes, serverless)
- **ğŸ”„ Intelligent Fallback**: Graceful degradation ensures 100% uptime even without LLM APIs
- **â˜ï¸ Multi-Cloud Support**: Deploy to AWS, GCP, or Azure with a single command
- **ğŸ”§ Infrastructure as Code**: Generates production-ready Terraform configurations
- **ğŸ³ Automatic Containerization**: Creates optimized Dockerfiles when needed
- **ğŸ“Š Comprehensive Logging**: Detailed logs of every deployment step
- **ğŸ’° Cost Estimation**: Provides monthly cost estimates before deployment
- **ğŸ“± QR Code Generation**: Scan to access your deployed app instantly
- **ğŸ¨ Modern Terminal UI**: Gradient banners, clickable links, and celebration animations

### Supported Deployment Strategies

- **Virtual Machines** (EC2, GCE, Azure VMs)
- **Containers** (ECS Fargate, Cloud Run)
- **Kubernetes** (EKS, GKE, AKS)
- **Serverless** (Lambda, Cloud Functions)

### Supported Services

- Managed databases (RDS, Cloud SQL, Azure Database)
- Redis/ElastiCache
- Load balancers
- Auto-scaling groups
- VPC networking
- SSL/TLS certificates

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     User Input Layer                        â”‚
â”‚  (Natural Language Description + GitHub Repo/Zip)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Analysis Layer                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚ Repository       â”‚      â”‚ NLP Requirement  â”‚           â”‚
â”‚  â”‚ Analyzer         â”‚      â”‚ Parser           â”‚           â”‚
â”‚  â”‚ - Detect app typeâ”‚      â”‚ - Parse intent   â”‚           â”‚
â”‚  â”‚ - Find deps      â”‚      â”‚ - Extract cloud  â”‚           â”‚
â”‚  â”‚ - Identify DB    â”‚      â”‚ - Determine type â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Decision Engine Layer                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Infrastructure Decision Engine               â”‚          â”‚
â”‚  â”‚ - Choose deployment strategy                 â”‚          â”‚
â”‚  â”‚ - Size compute resources                     â”‚          â”‚
â”‚  â”‚ - Configure networking                       â”‚          â”‚
â”‚  â”‚ - Select database options                    â”‚          â”‚
â”‚  â”‚ - Estimate costs                             â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Infrastructure Generation Layer                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Terraform Generator                          â”‚          â”‚
â”‚  â”‚ - Generate main.tf                           â”‚          â”‚
â”‚  â”‚ - Create variables.tf                        â”‚          â”‚
â”‚  â”‚ - Build outputs.tf                           â”‚          â”‚
â”‚  â”‚ - Generate user_data scripts                 â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Deployment Orchestration Layer                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Deployment Orchestrator                      â”‚          â”‚
â”‚  â”‚ 1. Clone repository                          â”‚          â”‚
â”‚  â”‚ 2. Build application (if needed)             â”‚          â”‚
â”‚  â”‚ 3. Create Dockerfile (if needed)             â”‚          â”‚
â”‚  â”‚ 4. Apply Terraform                           â”‚          â”‚
â”‚  â”‚ 5. Deploy application                        â”‚          â”‚
â”‚  â”‚ 6. Verify deployment                         â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Cloud Infrastructure                      â”‚
â”‚         (AWS / GCP / Azure Resources)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Terraform 1.0+
- Docker (for containerized deployments)
- Cloud provider credentials (AWS/GCP/Azure)
- **LLM API (Optional)**: System works with rule-based NLP by default
  - Vertex AI (uses GCP credentials) - Recommended if using GCP
  - OpenAI API key - For enhanced accuracy
  - Anthropic API key - Alternative option

### Installation

```bash
# Clone the repository
git clone https://github.com/iimahdii/arvo-autodeploy-takehome.git
cd arvo-autodeploy-takehome

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

### Environment Setup

Edit `.env` file with your credentials:

```bash
# LLM API Keys (OPTIONAL - System works with rule-based fallback)
# Priority: Vertex AI (uses GCP creds below) > OpenAI > Anthropic > Rule-based
OPENAI_API_KEY=sk-...           # Optional: OpenAI fallback
ANTHROPIC_API_KEY=sk-ant-...    # Optional: Anthropic fallback

# AWS Credentials (for AWS deployments)
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_DEFAULT_REGION=us-east-1

# GCP Credentials (for GCP deployments)
# Uses Service Account JSON Key (OAuth 2.0) - NOT simple API Keys
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
GCP_PROJECT_ID=your-project-id
GCP_PROJECT_NUMBER=your-project-number
GCP_REGION=us-central1          # Optional: for Vertex AI

# Note: If GCP credentials are set, Vertex AI will be used automatically
# No separate Vertex AI API key needed!
```

> **ğŸ” Authentication Note**: This system uses **Service Account JSON Keys** (OAuth 2.0) for GCP, not simple API Keys. This is more secure and recommended by Google. Organization policies that restrict API Key creation do not affect Service Account authentication.

### First Deployment

```bash
# Deploy the example application
python main.py deploy \
  --repo https://github.com/Arvo-AI/hello_world \
  --description "Deploy this Flask application on AWS"

# Or analyze without deploying
python main.py deploy \
  --repo https://github.com/Arvo-AI/hello_world \
  --description "Deploy this Flask application on AWS" \
  --dry-run
```

## ğŸ“š Usage Examples

### Example 1: Simple Flask App

```bash
python main.py deploy \
  --repo https://github.com/iimahdii/flask-app \
  --description "Deploy this Flask application on AWS using a t2.micro instance"
```

### Example 2: Django with Database

```bash
python main.py deploy \
  --repo https://github.com/iimahdii/django-app \
  --description "Deploy this Django application on AWS with PostgreSQL database in us-west-2"
```

### Example 3: Node.js with Auto-Scaling

```bash
python main.py deploy \
  --repo https://github.com/iimahdii/express-api \
  --description "Deploy this Express API on GCP with auto-scaling and load balancer"
```

### Example 4: Containerized Microservice

```bash
python main.py deploy \
  --repo https://github.com/iimahdii/microservice \
  --description "Deploy this containerized application on AWS ECS Fargate with Redis"
```

### Example 5: Local Repository

```bash
python main.py deploy \
  --repo /path/to/local/project \
  --description "Deploy this application on Azure with SSL"
```

### Example 6: Dry Run Analysis

```bash
python main.py deploy \
  --repo https://github.com/iimahdii/app \
  --description "Deploy on AWS" \
  --dry-run \
  --output-dir ./analysis
```

### Example 7: Repository Analysis Only

```bash
python main.py analyze https://github.com/iimahdii/app
```

## ğŸ” How It Works

### Phase 1: Repository Analysis

The **Repository Analyzer** performs deep inspection of your codebase:

1. **Language Detection**: Identifies primary programming language
2. **Framework Detection**: Recognizes Flask, Django, Express, React, etc.
3. **Dependency Parsing**: Reads `requirements.txt`, `package.json`, `Gemfile`, etc.
4. **Database Detection**: Identifies PostgreSQL, MySQL, MongoDB, Redis requirements
5. **Entry Point Discovery**: Finds `app.py`, `main.py`, `index.js`, etc.
6. **Command Inference**: Determines build and start commands
7. **Port Detection**: Extracts application port from config files
8. **Environment Variables**: Discovers required env vars from `.env.example`

**Confidence Scoring**: Each analysis receives a confidence score (0-1) based on:
- Framework detection certainty
- Entry point identification
- Dependency file presence
- Configuration completeness

### Phase 2: Natural Language Processing

The **Requirement Parser** understands deployment intent with **intelligent multi-tier fallback**:

**Tier 1: Vertex AI (Google Cloud Gemini)** - Priority if GCP credentials available:
- Uses Gemini models for natural language understanding
- Leverages existing GCP Service Account credentials
- 40x more cost-effective than alternatives ($0.00025/1K tokens)
- No separate API key required

**Tier 2: OpenAI/Anthropic** - Fallback if API keys provided:
- Uses GPT-4 or Claude to parse complex requirements
- Understands context and implicit requirements
- Handles ambiguous or incomplete descriptions

**Tier 3: Rule-Based Parsing** - Always available:
- Pattern matching for cloud providers (AWS, GCP, Azure)
- Keyword extraction for deployment types
- Region and instance type detection
- Scaling and SSL requirement inference
- 70-80% accuracy, zero dependencies

**Extracted Information**:
- Cloud provider preference
- Deployment type (VM, container, K8s, serverless)
- Region/zone
- Instance sizing
- Scaling requirements
- Custom domain and SSL needs
- Additional services (database, Redis, load balancer)

### Phase 3: Infrastructure Decision Engine

The **Decision Engine** makes intelligent infrastructure choices:

**Compute Strategy**:
- Simple apps (Flask, Express) â†’ VM or container
- Apps with databases â†’ Container for isolation
- Multi-service apps â†’ Kubernetes
- Event-driven apps â†’ Serverless

**Resource Sizing**:
- Analyzes application complexity
- Considers database requirements
- Factors in scaling needs
- Balances cost vs. performance

**Networking**:
- VPC creation for security
- Public/private subnet configuration
- Load balancer for scaled deployments
- SSL/TLS for production apps

**Database Configuration**:
- Managed database selection (RDS, Cloud SQL)
- Instance class based on app needs
- Backup and retention policies
- Multi-AZ for high availability (optional)

**Cost Estimation**:
- Computes monthly infrastructure costs
- Includes compute, database, networking
- Provides transparency before deployment

### Phase 4: Terraform Generation

The **Terraform Generator** creates infrastructure as code:

**Generated Files**:
- `main.tf`: Core infrastructure resources
- `variables.tf`: Configurable parameters
- `outputs.tf`: Deployment information
- `user_data.sh`: VM initialization scripts

**AWS Resources Created**:
- VPC with public/private subnets
- Internet Gateway and Route Tables
- Security Groups with proper ingress/egress
- EC2 instances or ECS Fargate tasks
- RDS database instances (if needed)
- ElastiCache Redis (if needed)
- Application Load Balancer (if needed)
- Auto Scaling Groups (if enabled)

**Best Practices**:
- Least privilege security groups
- Encrypted storage
- Backup retention
- Tagging for resource management
- Modular and reusable code

### Phase 5: Deployment Orchestration

The **Orchestrator** executes the deployment:

1. **Repository Preparation**
   - Clones Git repository
   - Extracts zip files
   - Copies local directories

2. **Application Build**
   - Runs build commands (npm build, etc.)
   - Compiles TypeScript/assets
   - Generates static files

3. **Containerization**
   - Creates optimized Dockerfile if missing
   - Multi-stage builds for production
   - Minimal base images
   - Proper layer caching

4. **Infrastructure Provisioning**
   - Terraform init
   - Terraform plan (review changes)
   - Terraform apply (create resources)
   - Capture outputs (IPs, endpoints)

5. **Automatic Code Adjustments** â­
   - Replaces `localhost` with deployed VM's public IP
   - Converts hardcoded URLs to relative paths
   - Scans HTML, JavaScript, TypeScript files
   - **Minimal user intervention** - fully automated
   - Addresses common deployment issues automatically

6. **Application Deployment**
   - VM: SCP files, SSH commands, systemd service
   - Container: Docker build, push to registry, deploy
   - Kubernetes: Apply manifests, configure ingress
   - Serverless: Package and deploy function

6. **Verification**
   - Health check endpoints
   - Service status validation
   - Log inspection
   - Performance baseline

### Phase 6: Post-Deployment

**Outputs Provided**:
- Public IP or load balancer DNS
- Application URL
- Database endpoints
- SSH access information
- Deployment logs
- Cost breakdown

**Generated Artifacts**:
- Terraform state files
- Deployment configuration
- Access credentials
- Rollback instructions

## ğŸ”§ Technical Deep Dive

### Repository Analyzer Implementation

**Key Classes**:
```python
class RepositoryAnalyzer:
    - _detect_language(): File extension analysis
    - _detect_framework(): Pattern matching + content inspection
    - _parse_dependencies(): Multi-format dependency parsing
    - _detect_database(): Dependency + keyword analysis
    - _find_entry_point(): Convention-based discovery
    - _calculate_confidence(): Multi-factor scoring
```

**Detection Algorithms**:
- **Framework Detection**: Weighted scoring system
  - File existence: +1 point
  - Import statements: +2 points
  - Threshold: >0 for positive match

- **Database Detection**: Two-phase approach
  1. Dependency analysis (psycopg2, mysql, etc.)
  2. Code scanning for DB keywords

- **Port Detection**: Priority order
  1. Environment files (.env)
  2. Configuration files (config.py, settings.py)
  3. Framework defaults

### NLP Requirement Parser

**NLP Integration**:
```python
class RequirementParser:
    - Multi-tier intelligent fallback system:
      1. Vertex AI (Gemini) - uses GCP Service Account
      2. OpenAI GPT-4 - if API key provided
      3. Anthropic Claude - if API key provided  
      4. Rule-based parsing - always available
    - Structured output with JSON schema
    - Temperature: 0.1 for consistency
    - Graceful degradation for 100% uptime
```

**Prompt Engineering**:
- Clear role definition (DevOps expert)
- Structured output format (JSON)
- Context inclusion (app analysis)
- Decision rules and defaults
- Examples for few-shot learning

**Rule-Based Fallback**:
- Regex patterns for cloud providers
- Keyword matching for deployment types
- Region pattern recognition
- Smart defaults based on app type

### Infrastructure Decision Engine

**Decision Matrix**:

| App Type | Has DB | Complexity | â†’ Strategy |
|----------|--------|------------|------------|
| Flask/Express | No | Low | VM |
| Flask/Express | Yes | Medium | Container |
| Django/Rails | Yes | Medium | Container |
| Microservices | Yes | High | Kubernetes |
| API Gateway | No | Low | Serverless |

**Cost Optimization**:
- Smallest viable instance types
- Single-AZ for development
- Spot instances (future)
- Reserved instances (future)

### Terraform Generator

**Template System**:
- Jinja2 templating for flexibility
- Conditional resource creation
- Variable parameterization
- Output extraction

**Security Hardening**:
- Minimal security group rules
- Private subnets for databases
- No hardcoded credentials
- Encrypted storage by default

### Deployment Orchestrator

**Error Handling**:
- Try-catch at each phase
- Detailed error messages
- Rollback capabilities (future)
- Comprehensive logging

**Idempotency**:
- Terraform state management
- Resource tagging
- Conditional creation

## âš™ï¸ Configuration

### Cloud Provider Setup

#### AWS
```bash
# Option 1: Environment variables
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
export AWS_DEFAULT_REGION=us-east-1

# Option 2: AWS CLI
aws configure

# Option 3: .env file
AWS_ACCESS_KEY_ID=your_key
AWS_SECRET_ACCESS_KEY=your_secret
```

#### GCP
```bash
# Create service account and download JSON key
gcloud iam service-accounts create autodeploy
gcloud iam service-accounts keys create key.json \
  --iam-account autodeploy@project.iam.gserviceaccount.com

# Set in .env
GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json
GCP_PROJECT_ID=your-project-id
```

#### Azure
```bash
# Login and get credentials
az login
az account show

# Set in .env
AZURE_SUBSCRIPTION_ID=...
AZURE_TENANT_ID=...
```

### LLM API Keys

**OpenAI**:
```bash
# Get key from https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...
```

**Anthropic**:
```bash
# Get key from https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...
```

## ğŸ› ï¸ Supported Technologies

### Languages
- Python (3.7+)
- JavaScript/Node.js
- TypeScript
- Ruby
- PHP
- Go
- Java

### Frameworks

**Python**:
- Flask
- Django
- FastAPI
- Pyramid

**JavaScript/TypeScript**:
- Express.js
- Next.js
- React
- Vue.js
- Angular
- NestJS

**Ruby**:
- Ruby on Rails
- Sinatra

**PHP**:
- Laravel
- Symfony

**Java**:
- Spring Boot
- Micronaut

### Databases
- PostgreSQL
- MySQL
- MongoDB
- Redis
- SQLite

### Cloud Providers
- AWS (Primary support)
- GCP (Beta)
- Azure (Beta)

## ğŸ’° Cost Estimation

### AWS Pricing Examples

**Micro Deployment** (t2.micro):
- EC2 t2.micro: $8.50/month
- RDS t3.micro: $15/month
- Data transfer: $5/month
- **Total: ~$28/month**

**Small Deployment** (t2.small):
- EC2 t2.small: $17/month
- RDS t3.small: $30/month
- Load Balancer: $16/month
- **Total: ~$63/month**

**Container Deployment** (ECS Fargate):
- Fargate (0.25 vCPU, 512MB): $15/month
- RDS t3.micro: $15/month
- ALB: $16/month
- **Total: ~$46/month**

*Prices are estimates and vary by region and usage*

## ğŸ› Troubleshooting

### Common Issues

**Issue**: "Terraform init failed"
```bash
# Solution: Check Terraform installation
terraform version

# Reinstall if needed
brew install terraform  # macOS
```

**Issue**: "AWS credentials not found"
```bash
# Solution: Verify credentials
aws sts get-caller-identity

# Or check .env file
cat .env | grep AWS
```

**Issue**: "Repository analysis failed"
```bash
# Solution: Ensure repository is accessible
git clone <repo-url>  # Test manually

# Check for required files
ls -la requirements.txt package.json
```

**Issue**: "â„¹ï¸ Vertex AI models require GCP billing (optional)"
```bash
# This is informational - system continues with rule-based parsing
# Vertex AI is optional enhancement (15-20% better accuracy)
# To enable: Set up GCP billing at console.cloud.google.com

# Alternative: Add OpenAI or Anthropic API key
echo "OPENAI_API_KEY=sk-..." >> .env

# Or continue with rule-based (works great for most cases)
```

### Debug Mode

```bash
# Enable verbose logging
export LOG_LEVEL=DEBUG

# Run with output directory
python main.py deploy \
  --repo <repo> \
  --description <desc> \
  --output-dir ./debug
```

## ğŸ§ª Development

### Project Structure

```
Take-Home-Arvo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analyzer/
â”‚   â”‚   â””â”€â”€ repo_analyzer.py      # Repository analysis
â”‚   â”œâ”€â”€ nlp/
â”‚   â”‚   â””â”€â”€ requirement_parser.py  # NLP processing
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”‚   â”œâ”€â”€ decision_engine.py     # Infrastructure decisions
â”‚   â”‚   â””â”€â”€ terraform_generator.py # Terraform generation
â”‚   â”œâ”€â”€ deployer/
â”‚   â”‚   â””â”€â”€ orchestrator.py        # Deployment orchestration
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py              # Logging utilities
â”‚       â””â”€â”€ validators.py          # Input validation
â”œâ”€â”€ main.py                        # CLI entry point
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env.example                   # Environment template
â””â”€â”€ README.md                      # This file
```

### Running Tests

```bash
# Install dev dependencies
pip install pytest pytest-cov

# Run tests (when implemented)
pytest tests/ -v

# With coverage
pytest tests/ --cov=src --cov-report=html
```

### Contributing

This is a technical assessment project, but feedback is welcome:

1. Code review comments
2. Architecture suggestions
3. Performance improvements
4. Security enhancements

## ğŸ“„ License

This project was created as an assessment for Mahdi Project.

## ğŸ™ Acknowledgments

- **Terraform** for infrastructure as code
- **Google Cloud Vertex AI** for Gemini LLM integration
- **OpenAI/Anthropic** for alternative LLM capabilities
- **Rich** library for beautiful terminal output
